{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful modules for notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdne.conf import DATASETDIR\n",
    "from pathlib import Path\n",
    "from tqdne.dataset import H5Dataset, RandomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusers import UNet1DModel\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pathlib import Path\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from tqdne.conf import OUTPUTDIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create very simple synthetic dataset\n",
    "\n",
    "t = (5501 // 16) * 16\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# path_train = DATASETDIR / Path(\"data_train.h5\")\n",
    "# path_test = DATASETDIR / Path(\"data_test.h5\")\n",
    "# train_dataset = H5Dataset(path_train, cut=t)\n",
    "# test_dataset = H5Dataset(path_test, cut=t)\n",
    "\n",
    "train_dataset = RandomDataset(1024*8)\n",
    "test_dataset = RandomDataset(512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res, high_res = train_dataset[0]\n",
    "low_res.shape, high_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 100\n",
    "time = np.arange(0, t)/fs\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(time ,low_res[0].numpy(), 'b')\n",
    "plt.plot(time, high_res[0].numpy(), 'r')\n",
    "plt.xlim(1, 5)\n",
    "plt.xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_low, batch_high = next(iter(train_loader))\n",
    "batch_low.shape, batch_high.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "\n",
    "# Unet parameters\n",
    "unet_params = {\n",
    "    \"sample_size\":t,\n",
    "    \"in_channels\":1,\n",
    "    \"out_channels\":1,\n",
    "    \"block_out_channels\":  (32, 32, 64 ),\n",
    "    \"down_block_types\": ('DownBlock1D', 'DownBlock1D', 'AttnDownBlock1D'),\n",
    "    \"up_block_types\": ('AttnUpBlock1D', 'UpBlock1D', 'UpBlock1D'),\n",
    "    \"mid_block_type\": 'UNetMidBlock1D',\n",
    "    \"extra_in_channels\" : 1\n",
    "}\n",
    "\n",
    "scheduler_params = {\n",
    "    \"beta_schedule\": \"linear\",\n",
    "    \"beta_start\": 0.0001,\n",
    "    \"beta_end\": 0.02,\n",
    "    \"num_train_timesteps\": 100,\n",
    "}\n",
    "\n",
    "optimizer_params = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lr_warmup_steps\": 500,\n",
    "    \"n_train\": len(train_dataset) // batch_size,\n",
    "    \"seed\": 0,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_epochs\": max_epochs,\n",
    "}\n",
    "\n",
    "trainer_params = {\n",
    "    # trainer parameters\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"precision\": \"32-true\",  \n",
    "    # Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
    "    # 16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
    "    # Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
    "    \"max_epochs\": max_epochs,\n",
    "    \"accelerator\": \"auto\",\n",
    "    \"devices\": \"auto\",\n",
    "    \"num_nodes\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = UNet1DModel(**unet_params)\n",
    "net.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inputs(low_res, high_res):\n",
    "    \"\"\"Build Unet inputs from low and high resolution data.\"\"\"\n",
    "    return torch.cat((low_res, high_res), dim=1)\n",
    "high_resn = torch.rand(batch_size, 1,t)\n",
    "\n",
    "inputs = to_inputs(batch_low, high_resn)\n",
    "timesteps = torch.LongTensor([150]*batch_size)\n",
    "print(inputs.shape)\n",
    "assert net(inputs, timesteps).sample.shape == batch_high.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler(**scheduler_params)\n",
    "scheduler.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(batch_high.shape)\n",
    "timesteps = torch.LongTensor([50]*batch_size)\n",
    "noisy_sig = scheduler.add_noise(batch_high, noise, timesteps)\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(time, noisy_sig[0,0].numpy(), 'b', label=\"noisy\")\n",
    "plt.plot(time, batch_high[0,0].numpy(), 'r',  label=\"original\")\n",
    "plt.xlim(1, 5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is probably wrong because of the conditioning\n",
    "# import tqdm\n",
    "\n",
    "# def sample(noise):\n",
    "#     sample = noise\n",
    "#     for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
    "#         # 1. predict noise residual\n",
    "#         with torch.no_grad():\n",
    "#             residual = net(sample, t).sample\n",
    "#         # 2. compute less noisy image and set x_t -> x_t-1\n",
    "#         sample = scheduler.step(residual, t, sample).prev_sample\n",
    "\n",
    "#     return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# sample = train_dataset[0]\n",
    "# sig = sample.unsqueeze(0)\n",
    "# print(sig.shape)\n",
    "# noise = torch.randn(sig[:,:1].shape)\n",
    "# timesteps = torch.LongTensor([150])\n",
    "# noisy_sig = scheduler.add_noise(sig[:,:1], noise, timesteps)\n",
    "# noisy_sig = torch.concat([noisy_sig, sig[:,1:]], dim=1)\n",
    "# noise_pred = net(noisy_sig, timesteps).sample\n",
    "# loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline, AudioPipelineOutput\n",
    "\n",
    "\n",
    "class DDPMPipeline1DCond(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for image generation.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Parameters:\n",
    "        unet ([`UNet1DModel`]):\n",
    "            A `UNet1DModel` to denoise the encoded audio latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image. Can be one of\n",
    "            [`DDPMScheduler`], or [`DDIMScheduler`].\n",
    "    \"\"\"\n",
    "    model_cpu_offload_seq = \"unet\"\n",
    "\n",
    "    def __init__(self, unet, scheduler):\n",
    "        super().__init__()\n",
    "        self.num_inference_steps = scheduler.num_train_timesteps\n",
    "        self.register_modules(unet=unet, scheduler=scheduler)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        low_res: torch.Tensor,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        num_inference_steps: Optional[int] = None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[AudioPipelineOutput, Tuple]:\n",
    "        r\"\"\"\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            batch_size (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 1000):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.ImagePipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is\n",
    "                returned where the first element is a list with the generated images\n",
    "        \"\"\"\n",
    "        batch_size, channels, t = low_res.shape\n",
    "        assert self.unet.config.in_channels == channels\n",
    "        # Sample gaussian noise to begin loop\n",
    "        sig_shape = low_res.shape\n",
    "        assert self.unet.config.extra_in_channels == channels\n",
    "\n",
    "\n",
    "        if self.device.type == \"mps\":\n",
    "            # randn does not work reproducibly on mps\n",
    "            sig = randn_tensor(sig_shape, generator=generator)\n",
    "            sig = sig.to(self.device)\n",
    "        else:\n",
    "            sig = randn_tensor(sig_shape, generator=generator, device=self.device)\n",
    "\n",
    "        # set step values\n",
    "        if num_inference_steps is None:\n",
    "            num_inference_steps = self.num_inference_steps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for t in self.progress_bar(self.scheduler.timesteps):\n",
    "\n",
    "            inputs = to_inputs(low_res, sig)\n",
    "\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(inputs, t).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1\n",
    "            sig = self.scheduler.step(model_output, t, sig, generator=generator).prev_sample\n",
    "\n",
    "\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sig,)\n",
    "\n",
    "        return AudioPipelineOutput(audios=sig)\n",
    "    \n",
    "pipeline = DDPMPipeline1DCond(net, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(low_res, pipeline):\n",
    "    # Sample some signaol from random noise (this is the backward diffusion process).\n",
    "    sig = pipeline(\n",
    "        low_res = low_res,\n",
    "        generator=torch.manual_seed(optimizer_params[\"seed\"]),\n",
    "    ).audios\n",
    "\n",
    "    return sig\n",
    "\n",
    "batch_low, batch_high = next(iter(train_loader))\n",
    "gen_high = evaluate(batch_low, pipeline)\n",
    "\n",
    "plt.plot(time, batch_high[0,0].numpy(), 'b', label=\"high res\")\n",
    "plt.plot(time, batch_low[0,0].numpy(), 'r', label=\"low res\")\n",
    "plt.plot(time, gen_high[0,0].numpy(), 'g', alpha=0.5, label=\"generated\")\n",
    "plt.legend()\n",
    "plt.xlim(1, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def to_inputs(low_res, high_res):\n",
    "    \"\"\"Build Unet inputs from low and high resolution data.\"\"\"\n",
    "    return torch.cat((low_res, high_res), dim=1)\n",
    "\n",
    "class LightningDDMP(pl.LightningModule):\n",
    "    \"\"\"A PyTorch Lightning module for training a diffusion model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : torch.nn.Module\n",
    "        A PyTorch neural network.\n",
    "    noise_scheduler : DDPMScheduler\n",
    "        A scheduler for adding noise to the clean images.\n",
    "    config : TrainingConfig\n",
    "        A dataclass containing the training configuration.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net: torch.nn.Module, noise_scheduler: DDPMScheduler, optimizer_params:dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = net\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.pipeline = DDPMPipeline1DCond(self.net, self.noise_scheduler)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    # def forward(self, x: torch.Tensor):\n",
    "    #     return self.net(x)\n",
    "    def evaluate(self, low_res):\n",
    "        # Sample some signaol from random noise (this is the backward diffusion process).\n",
    "        sig = self.pipeline(\n",
    "            low_res = low_res,\n",
    "            generator=torch.manual_seed(optimizer_params[\"seed\"]),\n",
    "        ).audios\n",
    "\n",
    "        return sig\n",
    "\n",
    "    def cross_entropy_loss(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def global_step(self, batch: List, batch_idx: int, train: bool = False):\n",
    "        low_res, high_res = batch\n",
    "\n",
    "        # Sample noise to add to the high_res\n",
    "        noise = torch.randn(high_res.shape).to(high_res.device)\n",
    "        batch_size = high_res.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each signal\n",
    "        timesteps = torch.randint(\n",
    "            0, self.noise_scheduler.num_train_timesteps, (batch_size,), device=high_res.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean high_res according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_hig_res = self.noise_scheduler.add_noise(high_res, noise, timesteps)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        inputs = to_inputs(low_res, noisy_hig_res)\n",
    "        noise_pred = self.net(inputs, timesteps, return_dict=False)[0]\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        if train:\n",
    "            self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        else:\n",
    "            self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, train_batch: List, batch_idx: int):\n",
    "        return self.global_step(train_batch, batch_idx, train=True)\n",
    "\n",
    "    def validation_step(self, val_batch: List, batch_idx: int):\n",
    "        return self.global_step(val_batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(net.parameters(), lr=self.optimizer_params[\"learning_rate\"])\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self.optimizer_params[\"lr_warmup_steps\"],\n",
    "            num_training_steps=(self.optimizer_params[\"n_train\"] * self.optimizer_params[\"max_epochs\"]),\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "model = LightningDDMP(net, scheduler, optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '1D-UNET'\n",
    "\n",
    "# 1. Wandb Logger\n",
    "wandb_logger = WandbLogger() # add project='projectname' to log to a specific project\n",
    "\n",
    "# 2. Learning Rate Logger\n",
    "lr_logger = LearningRateMonitor()\n",
    "# 3. Set Early Stopping\n",
    "early_stopping = EarlyStopping('val_loss', mode='min', patience=5)\n",
    "# 4. saves checkpoints to 'model_path' whenever 'val_loss' has a new min\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=OUTPUTDIR / Path(name), filename='{name}_{epoch}-{val_loss:.2f}',\n",
    "                                      monitor='val_loss', mode='min', save_top_k=5)\n",
    "\n",
    "(OUTPUTDIR/Path(name)).mkdir(parents=True, exist_ok=True)\n",
    "# Define Trainer\n",
    "trainer = pl.Trainer(**trainer_params, logger=wandb_logger, callbacks=[lr_logger, early_stopping, checkpoint_callback], \n",
    "                     default_root_dir=OUTPUTDIR/Path(name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqdne-GZO51An4-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
